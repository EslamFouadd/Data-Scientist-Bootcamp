{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7gJ7BksInetW"
      },
      "outputs": [],
      "source": [
        "# Merge the taxi_owners and taxi_veh tables setting a suffix\n",
        "taxi_own_veh = taxi_owners.merge(taxi_veh, on='vid', suffixes=('_own','_veh'))\n",
        "\n",
        "# Print the value_counts to find the most popular fuel_type\n",
        "print(taxi_own_veh['fuel_type'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the first few rows of the census_altered table to view the change\n",
        "print(census_altered[['ward']].head())\n",
        "\n",
        "# Merge the wards and census_altered tables on the ward column\n",
        "wards_census_altered = wards.merge(census_altered, on='ward')\n",
        "\n",
        "# Print the shape of wards_census_altered\n",
        "print('wards_census_altered table shape:', wards_census_altered.shape)"
      ],
      "metadata": {
        "id": "OavaHLIOnnU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# One-to-many classification\n"
      ],
      "metadata": {
        "id": "csmLgbyxnyQA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge the licenses and biz_owners table on account\n",
        "licenses_owners = licenses.merge(biz_owners, on='account')\n",
        "\n",
        "# Group the results by title then count the number of accounts\n",
        "counted_df = licenses_owners.groupby('title').agg({'account':'count'})\n",
        "\n",
        "# Sort the counted_df in desending order\n",
        "sorted_df = counted_df.sort_values(by='account', ascending=False)\n",
        "\n",
        "# Use .head() method to print the first few rows of sorted_df\n",
        "print(sorted_df.head())"
      ],
      "metadata": {
        "id": "LrLRFua_nnXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merging multiple DataFrames\n"
      ],
      "metadata": {
        "id": "EOt7mS-rn5oZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge the ridership and cal tables\n",
        "ridership_cal = ridership.merge(cal, on=['year','month','day'])"
      ],
      "metadata": {
        "id": "ImPMEZ2FnnZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge the ridership, cal, and stations tables\n",
        "ridership_cal_stations = ridership.merge(cal, on=['year','month','day']) \\\n",
        "\t\t\t\t\t\t\t.merge(stations, on='station_id')"
      ],
      "metadata": {
        "id": "iboTCQUDnnbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge the ridership, cal, and stations tables\n",
        "ridership_cal_stations = ridership.merge(cal, on=['year','month','day']) \\\n",
        "\t\t\t\t\t\t\t.merge(stations, on='station_id')\n",
        "\n",
        "# Create a filter to filter ridership_cal_stations\n",
        "filter_criteria = ((ridership_cal_stations['month'] == 7)\n",
        "                   & (ridership_cal_stations['day_type'] == 'Weekday')\n",
        "                   & (ridership_cal_stations['station_name'] == 'Wilson'))\n",
        "\n",
        "# Use .loc and the filter to select for rides\n",
        "print(ridership_cal_stations.loc[filter_criteria, 'rides'].sum())"
      ],
      "metadata": {
        "id": "iCsILD7pnndi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge licenses and zip_demo, on zip; and merge the wards on ward\n",
        "licenses_zip_ward = licenses.merge(zip_demo, on='zip') \\\n",
        "            \t\t\t.merge(wards, on='ward')\n",
        "\n",
        "# Print the results by alderman and show median income\n",
        "print(licenses_zip_ward.groupby('alderman').agg({'income':'median'}))"
      ],
      "metadata": {
        "id": "7LvM0FNroAcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge land_use and census and merge result with licenses including suffixes\n",
        "land_cen_lic = land_use.merge(census, on='ward') \\\n",
        "                    .merge(licenses, on='ward', suffixes=('_cen','_lic'))"
      ],
      "metadata": {
        "id": "GIEHOTPYoA1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge land_use and census and merge result with licenses including suffixes\n",
        "land_cen_lic = land_use.merge(census, on='ward') \\\n",
        "                    .merge(licenses, on='ward', suffixes=('_cen','_lic'))\n",
        "\n",
        "# Group by ward, pop_2010, and vacant, then count the # of accounts\n",
        "pop_vac_lic = land_cen_lic.groupby(['ward','pop_2010','vacant'],\n",
        "                                   as_index=False).agg({'account':'count'})"
      ],
      "metadata": {
        "id": "sLoq5FvToA3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge land_use and census and merge result with licenses including suffixes\n",
        "land_cen_lic = land_use.merge(census, on='ward') \\\n",
        "                    .merge(licenses, on='ward', suffixes=('_cen','_lic'))\n",
        "\n",
        "# Group by ward, pop_2010, and vacant, then count the # of accounts\n",
        "pop_vac_lic = land_cen_lic.groupby(['ward','pop_2010','vacant'],\n",
        "                                   as_index=False).agg({'account':'count'})\n",
        "\n",
        "# Sort pop_vac_lic and print the results\n",
        "sorted_pop_vac_lic = pop_vac_lic.sort_values(['vacant', 'account', 'pop_2010'],\n",
        "                                             ascending=[False, True, True])\n",
        "\n",
        "# Print the top few rows of sorted_pop_vac_lic\n",
        "print(sorted_pop_vac_lic.head())"
      ],
      "metadata": {
        "id": "FqQVzNTuoA5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Left Join"
      ],
      "metadata": {
        "id": "u4_461f7oQkA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge movies and financials with a left join\n",
        "movies_financials = movies.merge(financials, on='id', how='left')"
      ],
      "metadata": {
        "id": "l2V5bkDboA73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge the movies table with the financials table with a left join\n",
        "movies_financials = movies.merge(financials, on='id', how='left')\n",
        "\n",
        "# Count the number of rows in the budget column that are missing\n",
        "number_of_missing_fin = movies_financials['budget'].isnull().sum()\n",
        "\n",
        "# Print the number of movies missing financials\n",
        "print(number_of_missing_fin)"
      ],
      "metadata": {
        "id": "GgxUP0WRoAez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge the toy_story and taglines tables with a left join\n",
        "toystory_tag = toy_story.merge(taglines, on='id', how='left')\n",
        "\n",
        "# Print the rows and shape of toystory_tag\n",
        "print(toystory_tag)\n",
        "print(toystory_tag.shape)"
      ],
      "metadata": {
        "id": "AQO6h0-PoWuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge the toy_story and taglines tables with a inner join\n",
        "toystory_tag = toy_story.merge(taglines, on='id', how='inner')\n",
        "\n",
        "# Print the rows and shape of toystory_tag\n",
        "print(toystory_tag)\n",
        "print(toystory_tag.shape)"
      ],
      "metadata": {
        "id": "E4Gt_p4qoWwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The output of a one-to-many merge with a left join will have a greater than or equal rows than the left table.**"
      ],
      "metadata": {
        "id": "BfPJnD7KovG-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Right join to find unique movies\n",
        "\n",
        "Most of the recent big-budget science fiction movies can also be classified as action movies. You are given a table of science fiction movies called scifi_movies and another table of action movies called action_movies. Your goal is to find which movies are considered only science fiction movies. Once you have this table, you can merge the movies table in to see the movie names. Since this exercise is related to science fiction movies, use a right join as your superhero power to solve this problem.\n",
        "\n",
        "The movies, scifi_movies, and action_movies tables have been loaded for you."
      ],
      "metadata": {
        "id": "p8_DdlcUozaj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge action_movies to the scifi_movies with right join\n",
        "action_scifi = action_movies.merge(scifi_movies, on='movie_id', how='right',\n",
        "                                   suffixes=('_act','_sci'))\n",
        "\n",
        "# From action_scifi, select only the rows where the genre_act column is null\n",
        "scifi_only = action_scifi[action_scifi['genre_act'].isnull()]\n",
        "\n",
        "# Merge the movies and scifi_only tables with an inner join\n",
        "movies_and_scifi_only = movies.merge(scifi_only, how='inner',\n",
        "                                     left_on='id', right_on='movie_id')\n",
        "\n",
        "# Print the first few rows and shape of movies_and_scifi_only\n",
        "print(movies_and_scifi_only.head())\n",
        "print(movies_and_scifi_only.shape)"
      ],
      "metadata": {
        "id": "QRG0YPe7oWyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use right join to merge the movie_to_genres and pop_movies tables\n",
        "genres_movies = movie_to_genres.merge(pop_movies, how='right',\n",
        "                                      left_on='movie_id',\n",
        "                                      right_on='id')\n",
        "\n",
        "# Count the number of genres\n",
        "genre_count = genres_movies.groupby('genre').agg({'id':'count'})\n",
        "\n",
        "# Plot a bar chart of the genre_count\n",
        "genre_count.plot(kind='bar')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uvXwGa7koW0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using outer join to select actors\n",
        "\n",
        "![img](https://assets.datacamp.com/production/repositories/5486/datasets/c5d02ebba511e90ae132f89ff091e6729c040bd2/noJoin.png)\n",
        "\n",
        "One cool aspect of using an outer join is that, because it returns all rows from both merged tables and null where they do not match, you can use it to find rows that do not have a match in the other table. To try for yourself, you have been given two tables with a list of actors from two popular movies: Iron Man 1 and Iron Man 2. Most of the actors played in both movies. Use an outer join to find actors who did not act in both movies.\n",
        "\n",
        "The Iron Man 1 table is called iron_1_actors, and Iron Man 2 table is called iron_2_actors. Both tables have been loaded for you and a few rows printed so you can see the structure."
      ],
      "metadata": {
        "id": "oJS5wFRVpAz-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge iron_1_actors to iron_2_actors on id with outer join using suffixes\n",
        "iron_1_and_2 = iron_1_actors.merge(iron_2_actors,\n",
        "                                     on='id',\n",
        "                                     how='outer',\n",
        "                                     suffixes=('_1','_2'))\n",
        "\n",
        "# Create an index that returns true if name_1 or name_2 are null\n",
        "m = ((iron_1_and_2['name_1'].isnull()) |\n",
        "     (iron_1_and_2['name_2'].isnull()))\n",
        "\n",
        "# Print the first few rows of iron_1_and_2\n",
        "print(iron_1_and_2[m].head())"
      ],
      "metadata": {
        "id": "hIBjn2FcoW2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge the crews table to itself\n",
        "crews_self_merged = crews.merge(crews, on='id', how='inner',\n",
        "                                suffixes=('_dir','_crew'))\n",
        "\n",
        "# Create a boolean index to select the appropriate rows\n",
        "boolean_filter = ((crews_self_merged['job_dir'] == 'Director') &\n",
        "                  (crews_self_merged['job_crew'] != 'Director'))\n",
        "direct_crews = crews_self_merged[boolean_filter]\n",
        "\n",
        "# Print the first few rows of direct_crews\n",
        "print(direct_crews.head())"
      ],
      "metadata": {
        "id": "nHTS11-Ho9Zf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Merging on indexes"
      ],
      "metadata": {
        "id": "oexlWYxnpSuZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge to the movies table the ratings table on the index\n",
        "movies_ratings = movies.merge(ratings, on='id', how='left')\n",
        "\n",
        "# Print the first few rows of movies_ratings\n",
        "print(movies_ratings.head())"
      ],
      "metadata": {
        "id": "_mBq5_zOo9b1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge sequels and financials on index id\n",
        "sequels_fin = sequels.merge(financials, on='id', how='left')\n",
        "\n",
        "# Self merge with suffixes as inner join with left on sequel and right on id\n",
        "orig_seq = sequels_fin.merge(sequels_fin, how='inner', left_on='sequel',\n",
        "                             right_on='id', right_index=True,\n",
        "                             suffixes=('_org','_seq'))\n",
        "\n",
        "# Add calculation to subtract revenue_org from revenue_seq\n",
        "orig_seq['diff'] = orig_seq['revenue_seq'] - orig_seq['revenue_org']\n",
        "\n",
        "# Select the title_org, title_seq, and diff\n",
        "titles_diff = orig_seq[['title_org','title_seq','diff']]\n",
        "\n",
        "# Print the first rows of the sorted titles_diff\n",
        "print(titles_diff.sort_values('diff', ascending=False).head())"
      ],
      "metadata": {
        "id": "lCa6uOSuo9eG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Performing an anti join\n",
        "\n",
        "In our music streaming company dataset, each customer is assigned an employee representative to assist them. In this exercise, filter the employee table by a table of top customers, returning only those employees who are not assigned to a customer. The results should resemble the results of an anti join. The company's leadership will assign these employees additional training so that they can work with high valued customers.\n",
        "\n",
        "The top_cust and employees tables have been provided for you."
      ],
      "metadata": {
        "id": "yXP5h3qLxER0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge employees and top_cust\n",
        "empl_cust = employees.merge(top_cust, on='srid',\n",
        "                            how='left', indicator=True)\n",
        "\n",
        "# Select the srid column where _merge is left_only\n",
        "srid_list = empl_cust.loc[empl_cust['_merge'] == 'left_only', 'srid']\n",
        "\n",
        "# Get employees not working with top customers\n",
        "print(employees[employees['srid'].isin(srid_list)])"
      ],
      "metadata": {
        "id": "q2Jg2KSUoW4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Performing a semi join\n",
        "\n",
        "Some of the tracks that have generated the most significant amount of revenue are from TV-shows or are other non-musical audio. You have been given a table of invoices that include top revenue-generating items. Additionally, you have a table of non-musical tracks from the streaming service. In this exercise, you'll use a semi join to find the top revenue-generating non-musical tracks..\n",
        "\n",
        "The tables non_mus_tcks, top_invoices, and genres have been loaded for you."
      ],
      "metadata": {
        "id": "vj0R2hKExKJA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge the non_mus_tck and top_invoices tables on tid\n",
        "tracks_invoices = non_mus_tcks.merge(top_invoices, on='tid')\n",
        "\n",
        "# Use .isin() to subset non_mus_tcsk to rows with tid in tracks_invoices\n",
        "top_tracks = non_mus_tcks[non_mus_tcks['tid'].isin(tracks_invoices['tid'])]\n",
        "\n",
        "# Group the top_tracks by gid and count the tid rows\n",
        "cnt_by_gid = top_tracks.groupby(['gid'], as_index=False).agg({'tid':'count'})\n",
        "\n",
        "# Merge the genres table to cnt_by_gid on gid and print\n",
        "print(cnt_by_gid.merge(genres, on='gid'))"
      ],
      "metadata": {
        "id": "f6ohqekgoW6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Concatenation basics\n",
        "\n",
        "You have been given a few tables of data with musical track info for different albums from the metal band, Metallica. The track info comes from their Ride The Lightning, Master Of Puppets, and St. Anger albums. Try various features of the .concat() method by concatenating the tables vertically together in different ways.\n",
        "\n",
        "The tables tracks_master, tracks_ride, and tracks_st have loaded for you."
      ],
      "metadata": {
        "id": "_X4O-dBixQoZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenate the tracks, show only columns names that are in all tables\n",
        "tracks_from_albums = pd.concat([tracks_master, tracks_ride, tracks_st],\n",
        "                               join='inner',\n",
        "                               sort=True)\n",
        "print(tracks_from_albums)"
      ],
      "metadata": {
        "id": "PGTJjXQqxHI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Concatenating with keys\n",
        "\n",
        "The leadership of the music streaming company has come to you and asked you for assistance in analyzing sales for a recent business quarter. They would like to know which month in the quarter saw the highest average invoice total. You have been given three tables with invoice data named inv_jul, inv_aug, and inv_sep. Concatenate these tables into one to create a graph of the average monthly invoice total."
      ],
      "metadata": {
        "id": "y6mfa2_hxWV9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenate the tables and add keys\n",
        "inv_jul_thr_sep = pd.concat([inv_jul, inv_aug, inv_sep],\n",
        "                            keys=['7Jul','8Aug','9Sep'])\n",
        "\n",
        "# Group the invoices by the index keys and find avg of the total column\n",
        "avg_inv_by_month = inv_jul_thr_sep.groupby(level=0).agg({'total':'mean'})\n",
        "\n",
        "# Bar plot of avg_inv_by_month\n",
        "avg_inv_by_month.plot(kind='bar')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "C6Kn21VbxHLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Concatenate and merge to find common songs\n",
        "\n",
        "The senior leadership of the streaming service is requesting your help again. You are given the historical files for a popular playlist in the classical music genre in 2018 and 2019. Additionally, you are given a similar set of files for the most popular pop music genre playlist on the streaming service in 2018 and 2019. Your goal is to concatenate the respective files to make a large classical playlist table and overall popular music table. Then filter the classical music table using a semi join to return only the most popular classical music tracks.\n",
        "\n",
        "The tables classic_18, classic_19, and pop_18, pop_19 have been loaded for you. Additionally, pandas has been loaded as pd."
      ],
      "metadata": {
        "id": "NvhMtRW3xdDv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenate the classic tables vertically\n",
        "classic_18_19 = pd.concat([classic_18, classic_19], ignore_index=True)\n",
        "\n",
        "# Concatenate the pop tables vertically\n",
        "pop_18_19 = pd.concat([pop_18, pop_19], ignore_index=True)\n",
        "\n",
        "# Merge classic_18_19 with pop_18_19\n",
        "classic_pop = classic_18_19.merge(pop_18_19, on='tid')\n",
        "\n",
        "# Using .isin(), filter classic_18_19 rows where tid is in classic_pop\n",
        "popular_classic = classic_18_19[classic_18_19['tid'].isin(classic_pop['tid'])]\n",
        "\n",
        "# Print popular chart\n",
        "print(popular_classic)"
      ],
      "metadata": {
        "id": "2-YwdJMvxHMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Correlation between GDP and S&P500\n",
        "\n",
        "In this exercise, you want to analyze stock returns from the S&P 500. You believe there may be a relationship between the returns of the S&P 500 and the GDP of the US. Merge the different datasets together to compute the correlation.\n",
        "\n",
        "Two tables have been provided for you, named sp500, and gdp. As always, pandas has been imported for you as pd."
      ],
      "metadata": {
        "id": "IjZ69u8bxihl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use merge_ordered() to merge gdp and sp500, interpolate missing value\n",
        "gdp_sp500 = pd.merge_ordered(gdp, sp500, left_on='year', right_on='date',\n",
        "                             how='left',  fill_method='ffill')\n",
        "\n",
        "# Subset the gdp and returns columns\n",
        "gdp_returns = gdp_sp500[['gdp','returns']]\n",
        "\n",
        "# Print gdp_returns correlation\n",
        "print(gdp_returns.corr())"
      ],
      "metadata": {
        "id": "f5JqgiibxHOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phillips curve using merge_ordered()\n",
        "\n",
        "There is an economic theory developed by A. W. Phillips which states that inflation and unemployment have an inverse relationship. The theory claims that with economic growth comes inflation, which in turn should lead to more jobs and less unemployment.\n",
        "\n",
        "You will take two tables of data from the U.S. Bureau of Labor Statistics, containing unemployment and inflation data over different periods, and create a Phillips curve. The tables have different frequencies. One table has a data entry every six months, while the other has a data entry every month. You will need to use the entries where you have data within both tables.\n",
        "\n",
        "The tables unemployment and inflation have been loaded for you."
      ],
      "metadata": {
        "id": "c1FH-57jxsX8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use merge_ordered() to merge inflation, unemployment with inner join\n",
        "inflation_unemploy = pd.merge_ordered(inflation, unemployment,\n",
        "                                      on='date', how='inner')\n",
        "\n",
        "# Print inflation_unemploy\n",
        "print(inflation_unemploy)\n",
        "\n",
        "# Plot a scatter plot of unemployment_rate vs cpi of inflation_unemploy\n",
        "inflation_unemploy.plot(kind='scatter', x='unemployment_rate', y='cpi')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "F44IK4mcxHQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# merge_ordered() caution, multiple columns\n",
        "\n",
        "When using merge_ordered() to merge on multiple columns, the order is important when you combine it with the forward fill feature. The function sorts the merge on columns in the order provided. In this exercise, we will merge GDP and population data from the World Bank for the Australia and Sweden, reversing the order of the merge on columns. The frequency of the series are different, the GDP values are quarterly, and the population is yearly. Use the forward fill feature to fill in the missing data. Depending on the order provided, the fill forward will use unintended data to fill in the missing values.\n",
        "\n",
        "The tables gdp and pop have been loaded."
      ],
      "metadata": {
        "id": "Se4j89VgxyrE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge gdp and pop on country and date with fill\n",
        "date_ctry = pd.merge_ordered(gdp, pop, on=['country','date'],\n",
        "                             fill_method='ffill')\n",
        "\n",
        "# Print date_ctry\n",
        "print(date_ctry)"
      ],
      "metadata": {
        "id": "IAEleAgfoAg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using merge_asof() to study stocks\n",
        "\n",
        "You have a feed of stock market prices that you record. You attempt to track the price every five minutes. Still, due to some network latency, the prices you record are roughly every 5 minutes. You pull your price logs for three banks, JP Morgan (JPM), Wells Fargo (WFC), and Bank Of America (BAC). You want to know how the price change of the two other banks compare to JP Morgan. Therefore, you will need to merge these three logs into one table. Afterward, you will use the pandas .diff() method to compute the price change over time. Finally, plot the price changes so you can review your analysis.\n",
        "\n",
        "The three log files have been loaded for you as tables named jpm, wells, and bac."
      ],
      "metadata": {
        "id": "tQ65Th0fx2mE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use merge_asof() to merge jpm and wells\n",
        "jpm_wells = pd.merge_asof(jpm, wells, on='date_time',\n",
        "                          suffixes=('', '_wells'), direction='nearest')\n",
        "\n",
        "# Use merge_asof() to merge jpm_wells and bac\n",
        "jpm_wells_bac = pd.merge_asof(jpm_wells, bac, on='date_time',\n",
        "                              suffixes=('_jpm', '_bac'), direction='nearest')\n",
        "\n",
        "# Compute price diff\n",
        "price_diffs = jpm_wells_bac.diff()\n",
        "\n",
        "# Plot the price diff of the close of jpm, wells and bac only\n",
        "price_diffs.plot(y=['close_jpm','close_wells','close_bac'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZFEs7v28xuVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Using merge_asof() to create dataset\n",
        "\n",
        "The merge_asof() function can be used to create datasets where you have a table of start and stop dates, and you want to use them to create a flag in another table. You have been given gdp, which is a table of quarterly GDP values of the US during the 1980s. Additionally, the table recession has been given to you. It holds the starting date of every US recession since 1980, and the date when the recession was declared to be over. Use merge_asof() to merge the tables and create a status flag if a quarter was during a recession. Finally, to check your work, plot the data in a bar chart.\n",
        "\n",
        "The tables gdp and recession have been loaded for you."
      ],
      "metadata": {
        "id": "qxlRXclJyCvV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge gdp and recession on date using merge_asof()\n",
        "gdp_recession = pd.merge_asof(gdp, recession, on='date')\n",
        "\n",
        "# Create a list based on the row value of gdp_recession['econ_status']\n",
        "is_recession = ['r' if s=='recession' else 'g' for s in gdp_recession['econ_status']]\n",
        "\n",
        "# Plot a bar chart of gdp_recession\n",
        "gdp_recession.plot(kind='bar', y='gdp', x='date', color=is_recession, rot=90)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9wV_qECdxuXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Subsetting rows with .query()\n",
        "\n",
        "In this exercise, you will revisit GDP and population data for Australia and Sweden from the World Bank and expand on it using the .query() method. You'll merge the two tables and compute the GDP per capita. Afterwards, you'll use the .query() method to sub-select the rows and create a plot. Recall that you will need to merge on multiple columns in the proper order.\n",
        "\n",
        "The tables gdp and pop have been loaded for you."
      ],
      "metadata": {
        "id": "RO_4BqrJyLgR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge gdp and pop on date and country with fill\n",
        "gdp_pop = pd.merge_ordered(gdp, pop, on=['country','date'], fill_method='ffill')\n",
        "\n",
        "# Add a column named gdp_per_capita to gdp_pop that divides the gdp by pop\n",
        "gdp_pop['gdp_per_capita'] = gdp_pop['gdp'] / gdp_pop['pop']\n",
        "\n",
        "# Pivot data so gdp_per_capita, where index is date and columns is country\n",
        "gdp_pivot = gdp_pop.pivot_table('gdp_per_capita', 'date', 'country')\n",
        "\n",
        "# Select dates equal to or greater than 1991-01-01\n",
        "recent_gdp_pop = gdp_pivot.query('date >= \"1991-01-01\"')\n",
        "\n",
        "# Plot recent_gdp_pop\n",
        "recent_gdp_pop.plot(rot=90)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gpT7xL2ExuZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using .melt() to reshape government data\n",
        "\n",
        "The US Bureau of Labor Statistics (BLS) often provides data series in an easy-to-read format - it has a separate column for each month, and each year is a different row. Unfortunately, this wide format makes it difficult to plot this information over time. In this exercise, you will reshape a table of US unemployment rate data from the BLS into a form you can plot using .melt(). You will need to add a date column to the table and sort by it to plot the data correctly.\n",
        "\n",
        "The unemployment rate data has been loaded for you in a table called ur_wide. You are encouraged to view the table in the IPython shell before beginning the exercise."
      ],
      "metadata": {
        "id": "tnni6t2dyUgh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Unpivot everything besides the year column\n",
        "ur_tall = ur_wide.melt(id_vars=['year'], var_name='month',\n",
        "                       value_name='unempl_rate')\n",
        "\n",
        "# Create a date column using the month and year columns of ur_tall\n",
        "ur_tall['date'] = pd.to_datetime(ur_tall['month'] + '-' + ur_tall['year'])\n",
        "\n",
        "# Sort ur_tall by date in ascending order\n",
        "ur_sorted = ur_tall.sort_values('date')\n",
        "\n",
        "# Plot the unempl_rate by date\n",
        "ur_sorted.plot(x='date', y='unempl_rate')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cZOpiTbWyFwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using .melt() for stocks vs bond performance\n",
        "\n",
        "It is widespread knowledge that the price of bonds is inversely related to the price of stocks. In this last exercise, you'll review many of the topics in this chapter to confirm this. You have been given a table of percent change of the US 10-year treasury bond price. It is in a wide format where there is a separate column for each year. You will need to use the .melt() method to reshape this table.\n",
        "\n",
        "Additionally, you will use the .query() method to filter out unneeded data. You will merge this table with a table of the percent change of the Dow Jones Industrial stock index price. Finally, you will plot data.\n",
        "\n",
        "The tables ten_yr and dji have been loaded for you."
      ],
      "metadata": {
        "id": "CljTLiiLyZbc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use melt on ten_yr, unpivot everything besides the metric column\n",
        "bond_perc = ten_yr.melt(id_vars='metric', var_name='date', value_name='close')\n",
        "\n",
        "# Use query on bond_perc to select only the rows where metric=close\n",
        "bond_perc_close = bond_perc.query('metric == \"close\"')\n",
        "\n",
        "# Merge (ordered) dji and bond_perc_close on date with an inner join\n",
        "dow_bond = pd.merge_ordered(dji, bond_perc_close, on='date',\n",
        "                            suffixes=('_dow', '_bond'), how='inner')\n",
        "\n",
        "# Plot only the close_dow and close_bond columns\n",
        "dow_bond.plot(y=['close_dow', 'close_bond'], x='date', rot=90)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wyLMRW-cyFzH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}